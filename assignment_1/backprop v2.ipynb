{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACML Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "\n",
    "for i in range(8):\n",
    "    input = np.zeros(((8+1),1)) # +1 is the bias\n",
    "    input[0] = 1 # bias at the beginning of the input\n",
    "    input[i+1] = 1\n",
    "    inputs.append(input)\n",
    "\n",
    "#print(inputs[0:2])\n",
    "#len(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "W1 = np.random.normal(0, 0.1, 3*9).reshape(3, 9) # 3x9 matrix\n",
    "W2 = np.random.normal(0, 0.1, 8*4).reshape(8, 4) # 8x4 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the weight adjustment\n",
    "lr = 0.1\n",
    "batch_size = 8\n",
    "weight_decay = 0.000001\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1 + np.exp(-x)))\n",
    "\n",
    "def softmax(x):\n",
    "    e = np.exp(x)\n",
    "    return(e/e.sum())\n",
    "\n",
    "def argmax(x): \n",
    "    a2_index = np.argmax(x)\n",
    "    a2 = np.zeros(8)\n",
    "    a2[a2_index] = 1\n",
    "    return(a2)\n",
    "\n",
    "\n",
    "# forward propagation function\n",
    "# a0 is the input layer, which must be of shape (9,1) (8+1)\n",
    "# returns the activations of layer 1 and 2\n",
    "def forward_prop(a0):\n",
    "    a1 = sigmoid(np.dot(W1, a0)) # (3x9) . (9x1) = (3x1)\n",
    "    a1 = np.insert(a1, 0, 1) # insert bias to make it (4x1)\n",
    "    a2 = sigmoid(np.dot(W2, a1))\n",
    "    return(a1, a2) # returns (4x1) and (8x1)\n",
    "\n",
    "# back propagation function\n",
    "# inputs are: a0 (=y), a1, a2 (activation of layer 1 and 2)\n",
    "# outputs are:\n",
    "def back_prop(a0, a1, a2):\n",
    "    \n",
    "    # compute the costs (squared error)\n",
    "    cost_MSE = np.sum(np.square(a2 - a0))\n",
    "\n",
    "    # compute the deltas for layer 2 and 1\n",
    "    delta2 = a2 * (np.ones(8) - a2) * (a2 - a0) # deltas in layer 2 (output)\n",
    "    #       (8x1) * ((8x1) - (8x1)) * ((8x1)-(8x1)) = (8x1)\n",
    "\n",
    "    #print(\"a2\", a2)\n",
    "    #print(\"a0\", a0)\n",
    "\n",
    "    #print(\"W2 transposed\", W2.T)\n",
    "    delta1 = a1 * (np.ones(4) - a1) * np.dot(W2.T, delta2) # deltas in layer 1 (hidden)\n",
    "    #         (4x1) * ((4x1) - (4x1))  *  ((4x8) . (8x1))   = (4x1)\n",
    "    #                                          (4x1)\n",
    "    #print(a2)\n",
    "    #print(a2.reshape(-1,1))\n",
    "\n",
    "    # compute the partial derivatives for weigthts 2\n",
    "    #print(\"a1\", a1.reshape(4,1))\n",
    "    #print(\"delta2\", delta2)\n",
    "    deriv_W_2 = np.dot(delta2.reshape(8,1), a1.reshape(1,4))\n",
    "\n",
    "\n",
    "    # compute the partial derivatives for weights 1\n",
    "    # insert bias to a0\n",
    "    a0_with_bias = np.insert(a0, 0, 1)\n",
    "    a0_with_bias = a0_with_bias.reshape(1, 9)\n",
    "    \n",
    "    # delete bias for delta1\n",
    "    delta1 = np.delete(delta1, 0)\n",
    "    #print(a0_with_bias)\n",
    "    #print(delta1)\n",
    "    deriv_W_1 = np.dot(delta1.reshape(3, 1), a0_with_bias)\n",
    "\n",
    "    #print(deriv_W_1.shape, deriv_W_2.shape)   \n",
    "    return(deriv_W_1, deriv_W_2, cost_MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[0])\n",
    "\n",
    "# activations of layer 1 and 2\n",
    "ta1, ta2 = forward_prop(inputs[0])\n",
    "\n",
    "y = np.delete(inputs[0], 0)\n",
    "print(\"y\", y)\n",
    "der1, der2, cost= back_prop(y, ta1, ta2)\n",
    "\n",
    "print(\"activations of layer 1 and 2\")\n",
    "print(ta1, ta2)\n",
    "print(\"derivatives of the weights W1 and W2\")\n",
    "der1, der2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = list()\n",
    "# loop over number of iterations\n",
    "for i in range(epochs):\n",
    "\n",
    "    # loop over batch size\n",
    "    deriv1_outer = np.zeros((3,9))\n",
    "    deriv2_outer = np.zeros((8,4))\n",
    "    for m in range(batch_size): #batch size 8\n",
    "        \n",
    "        # select a random training sample\n",
    "        train_sample = inputs[np.random.randint(0, 8)]\n",
    "        \n",
    "        # perform forward propagation\n",
    "        ta1, ta2 = forward_prop(train_sample)\n",
    "        \n",
    "        # perform backward propagation\n",
    "        y = np.delete(train_sample, 0)\n",
    "        deriv1, deriv2, cost = back_prop(y, ta1, ta2)\n",
    "        \n",
    "        # add the partial derivatives to the batch derivatives\n",
    "        #print(deriv1.shape, deriv1_outer.shape)\n",
    "        deriv1_outer = deriv1_outer + deriv1\n",
    "        deriv2_outer = deriv2_outer + deriv2\n",
    "        loss.append(cost)\n",
    "\n",
    "    W1_new = W1 - (lr * ((1/batch_size) * deriv1_outer + weight_decay * W1))\n",
    "    W2_new = W2 - (lr * ((1/batch_size) * deriv2_outer + weight_decay * W2))\n",
    "    W1_new[:,0] = W1[:,0] - lr * (1/batch_size) * deriv1_outer[:,0]\n",
    "    W2_new[:,0] = W2[:,0] - lr * (1/batch_size) * deriv2_outer[:,0]\n",
    "    W1 = W1_new\n",
    "    W2 = W2_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2\n",
    "W1[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot((list(range(len(loss)))), loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[1])\n",
    "\n",
    "_, output = forward_prop(inputs[1])\n",
    "\n",
    "print(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc724f59a171e05a5dc82d36a6cb3a410496cec6883807e37792edd8babbea61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
